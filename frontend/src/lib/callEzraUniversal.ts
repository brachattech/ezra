type ModelType = 'auto' | 'scout' | 'bert' | 'cnn' | 'llama' | 'gpt4turbo' | 'qwen-coder'

interface EzraOptions {
  prompt: string
  model?: ModelType // default = 'auto'
}

export async function callEzraUniversal({ prompt, model = 'auto' }: EzraOptions): Promise<string> {
  const systemPrompt = `
Voc√™ √© Ezra, um assistente executivo com intelig√™ncia compar√°vel ao GPT-4.

Sua fun√ß√£o √© gerar documentos formais com excel√™ncia t√©cnica e padr√£o internacional. Utilize sempre o portugu√™s formal, organizado, direto e preciso, sem s√≠mbolos ou hashtags.

1. Relat√≥rio Jur√≠dico:
- Comece com uma introdu√ß√£o explicando o objetivo do relat√≥rio.
- Apresente a fundamenta√ß√£o legal com leis nacionais e internacionais relevantes.
- Analise cl√°usulas, situa√ß√µes ou fatos com embasamento t√©cnico.
- Aponte riscos jur√≠dicos, poss√≠veis san√ß√µes e impactos.
- Finalize com uma conclus√£o e recomenda√ß√µes jur√≠dicas concretas.

2. Relat√≥rio Pedag√≥gico:
- Estruture com: Introdu√ß√£o, Indicadores observados, Avalia√ß√µes realizadas, Recomenda√ß√µes e Progn√≥stico.
- Descreva claramente as dificuldades do aluno, sem ju√≠zo de valor.
- Use termos t√©cnicos da √°rea educacional e pedag√≥gica.

3. An√°lise Contratual:
- Inicie com o escopo da an√°lise.
- Detalhe cl√°usulas cr√≠ticas, riscos, obriga√ß√µes e inconsist√™ncias.
- Apresente parecer t√©cnico claro, separado por t√≥picos.
- Finalize com conclus√µes e recomenda√ß√µes jur√≠dicas.

Nunca use hashtags. Nunca insira linguagem informal. Nunca misture √°reas. Organize os textos com t√≠tulos e subt√≠tulos claros. O foco √© sempre entregar um documento t√©cnico, internacionalmente padronizado.
  `.trim()

  function normalize(text: string): string {
    return text
      .normalize('NFD')
      .replace(/[\u0300-\u036f]/g, '')
      .replace(/√ß/g, 'c')
      .replace(/[^a-z0-9\s]/gi, '')
      .toLowerCase()
  }

  const normalizedPrompt = normalize(prompt)

  const shouldUseGPT4 = model === 'deepseek-ai/DeepSeek-R1' || (
    model === 'auto' &&
    (
      normalizedPrompt.includes('faca um relatorio juridico') ||
      normalizedPrompt.includes('faca um relatorio pedagogico') ||
      normalizedPrompt.includes('faca uma analise juridica')
    )
  )

  const shouldUseBert = model === 'bert' || (
    model === 'auto' && normalizedPrompt.includes('classifique')
  )

  const shouldUseCnn = model === 'cnn' || (
    model === 'auto' && normalizedPrompt.includes('olhe')
  )

  const shouldUseQwenCoder = model === 'qwen-coder' || (
    model === 'auto' &&
    (
      normalizedPrompt.includes('vamos codar') ||
      normalizedPrompt.includes('gere o codigo') ||
      normalizedPrompt.includes('refatore') ||
      normalizedPrompt.includes('typescript') ||
      normalizedPrompt.includes('python') ||
      normalizedPrompt.includes('next') ||
      normalizedPrompt.includes('react') ||
      normalizedPrompt.includes('fastapi')
    )
  )

  let selectedModel: string
  let fallbackModel: string | null = null

  if (shouldUseGPT4) {
    selectedModel = 'deepseek-ai/DeepSeek-R1'
  } else if (shouldUseBert) {
    selectedModel = 'togethercomputer/m2-bert-80M-32k-retrieval'
  } else if (shouldUseCnn) {
    selectedModel = 'black-forest-labs/FLUX1.1-pro'
  } else if (shouldUseQwenCoder) {
    selectedModel = 'Qwen/Qwen2.5-Coder-32B-Instruct'
    fallbackModel = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'
  } else {
    selectedModel = 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo'
    fallbackModel = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'
  }

  const approxPromptTokens = Math.ceil((prompt.length + systemPrompt.length) / 4)
  const maxModelTokens = 8192
  const targetResponseTokens = normalizedPrompt.includes('relatorio') ? 6000 : 1500

  let availableTokens: number
  if (selectedModel.includes('bert')) {
    availableTokens = 20
  } else if (selectedModel.includes('FLUX') || selectedModel.includes('flux')) {
    availableTokens = 1
  } else {
    availableTokens = Math.max(512, Math.min(targetResponseTokens, maxModelTokens - approxPromptTokens))
  }

  async function tryFetch(modelToUse: string): Promise<string> {
    if (modelToUse.includes('FLUX') || modelToUse.includes('flux')) {
      return '[üñºÔ∏è Modelo de imagem ativado. A resposta visual ser√° tratada separadamente.]'
    }

    const isOpenAI = modelToUse.startsWith('openai:') || modelToUse === 'gpt-4-turbo'
const proxyEndpoint = isOpenAI ? '/api/openai-proxy' : '/api/together-proxy'
const modelName = isOpenAI ? modelToUse.replace('openai:', '') : modelToUse


    const response = await fetch(proxyEndpoint, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelName,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: prompt }
        ],
        temperature: 0.4,
        max_tokens: availableTokens,
        top_p: 0.9
      }),
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error(`[Ezra Proxy] Erro HTTP (${modelName}):`, errorText)
      throw new Error('Erro na requisi√ß√£o')
    }

    const data = await response.json()
    return data.choices?.[0]?.message?.content ?? '[‚ö†Ô∏è Resposta vazia]'
  }

  try {
    const resposta = await tryFetch(selectedModel)
    return `${selectedModel}:::${resposta}`
  } catch (error: any) {
    console.error(`[Ezra LLM] Erro no modelo ${selectedModel}:`, error.message)

    if (fallbackModel) {
      try {
        console.warn(`[Ezra LLM] Tentando fallback: ${fallbackModel}`)
        const respostaFallback = await tryFetch(fallbackModel)
        return `${fallbackModel}:::${respostaFallback}`
      } catch (fallbackError: any) {
        console.error(`[Ezra LLM] Fallback tamb√©m falhou (${fallbackModel}):`, fallbackError.message)
      }
    }

    return '[‚ùå Falha ao processar resposta com Ezra]'
  }
}

